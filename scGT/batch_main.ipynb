{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca05748e-c83f-4550-8ea6-2985cf19d235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import os, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import to_undirected, remove_self_loops, add_self_loops, subgraph, k_hop_subgraph\n",
    "from torch_scatter import scatter\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "# 因为修改了dataset文件，直接import得到的一直是之前未修改的代码，所以要reload一下，先import，再reload\n",
    "import dataset\n",
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "\n",
    "import logger\n",
    "importlib.reload(logger)\n",
    "\n",
    "from logger import Logger\n",
    "from dataset import input_dataset\n",
    "from data_utils import load_fixed_splits, adj_mul, get_gpu_memory_map, to_sparse_tensor\n",
    "\n",
    "import eval\n",
    "importlib.reload(eval)\n",
    "\n",
    "from eval import evaluate_cpu, eval_acc, eval_rocauc, eval_f1\n",
    "from parse import parse_method, parser_add_main_args\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NOTE: for consistent data splits, see data_utils.rand_train_test_idx\n",
    "def fix_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85c4d976-cb55-48d6-b3db-a448a23936ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(K=15, M=35, batch_size=3000, cached=False, cpu=False, data_dir='../data/asaprpca/', dataset='data', device=0, directed=False, dropout=0.0, epochs=5000, eval_step=10, gat_heads=2, gpr_alpha=0.1, hidden_channels=128, hops=1, jk_type='max', knn_num=5, label_num_per_class=20, lamda=0.4, lp_alpha=0.1, lr=0.0001, method='nodeformer', metric='acc', model_dir='../model/', num_batch=2, num_heads=1, num_layers=2, num_mlp_layers=1, out_heads=1, projection_matrix_type=True, protocol='semi', rand_split=True, rand_split_class=False, rb_order=2, rb_trans='sigmoid', runs=1, save_model=True, seed=42, sub_dataset='', tau=0.25, train_prop=1, use_act=False, use_bn=True, use_gumbel=True, use_jk=False, use_residual=True, valid_prop=0, weight_decay=0.001)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "from nodeformer import *\n",
    "from data_utils import normalize\n",
    "\n",
    "default_args = {\n",
    "    'method': 'nodeformer',\n",
    "    'dataset': 'data',\n",
    "    'sub_dataset': '',\n",
    "    'data_dir': '../data/asaprpca/', # 标明数据的位置\n",
    "    'device': 0,\n",
    "    'seed': 42,\n",
    "    'epochs': 5000,\n",
    "    'eval_step': 10,\n",
    "    'cpu': False,\n",
    "    'runs': 1,\n",
    "    'train_prop': 1,\n",
    "    'valid_prop': 0,\n",
    "    'protocol': 'semi',\n",
    "    'rand_split': True,\n",
    "    'rand_split_class': False,\n",
    "    'label_num_per_class': 20,\n",
    "    'metric': 'acc',\n",
    "    'knn_num': 5,\n",
    "    'save_model': True,\n",
    "    'model_dir': '../model/',\n",
    "    'hidden_channels': 128,\n",
    "    'dropout': 0.0,\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 1e-3,\n",
    "    'num_layers': 2,\n",
    "    'num_heads': 1,\n",
    "    'M': 35,\n",
    "    'use_gumbel': True,\n",
    "    'use_residual': True,\n",
    "    'use_bn': True,\n",
    "    'use_act': False,\n",
    "    'use_jk': False,\n",
    "    'K': 15,\n",
    "    'tau': 0.25,\n",
    "    'lamda': 0.4,\n",
    "    'rb_order': 2,\n",
    "    'rb_trans': 'sigmoid',\n",
    "    'batch_size': 3000,\n",
    "    'hops': 1,\n",
    "    'cached': False,\n",
    "    'gat_heads': 2,\n",
    "    'out_heads': 1,\n",
    "    'projection_matrix_type': True,\n",
    "    'lp_alpha': 0.1,\n",
    "    'gpr_alpha': 0.1,\n",
    "    'directed': False,\n",
    "    'jk_type': 'max',\n",
    "    'num_mlp_layers': 1,\n",
    "    'num_batch': 2\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(**default_args)\n",
    "print(args)\n",
    "\n",
    "##################### 上面都是设置命令行参数 #####################\n",
    "fix_seed(args.seed)\n",
    "\n",
    "if args.cpu:\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c77b3caf-0f18-45c2-b757-7716f2eab432",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.45 s, sys: 3.73 s, total: 11.2 s\n",
      "Wall time: 7.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "### Load and preprocess data ###\n",
    "dataset = input_dataset(args.data_dir, args.dataset)\n",
    "\n",
    "if len(dataset.label.shape) == 1:\n",
    "    dataset.label = dataset.label.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a241b3b-9a9f-42fe-b153-b1cd364bc0d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num nodes 8801 | num edges 51905 | num classes 7 | num node feats 14530\n"
     ]
    }
   ],
   "source": [
    "# get the splits for all runs\n",
    "if args.rand_split:\n",
    "    split_idx_lst = [dataset.get_idx_split(train_prop=args.train_prop, valid_prop=args.valid_prop)\n",
    "                     for _ in range(args.runs)]\n",
    "elif args.rand_split_class:\n",
    "    split_idx_lst = [dataset.get_idx_split(split_type='class', label_num_per_class=args.label_num_per_class)\n",
    "                     for _ in range(args.runs)]\n",
    "elif args.dataset in ['ogbn-proteins', 'ogbn-arxiv', 'ogbn-products', 'amazon2m']:\n",
    "    split_idx_lst = [dataset.load_fixed_splits()\n",
    "                     for _ in range(args.runs)]\n",
    "else:\n",
    "    split_idx_lst = load_fixed_splits(args.data_dir, dataset, dataset=args.dataset, protocol=args.protocol)\n",
    "\n",
    "n = dataset.graph['num_nodes']\n",
    "# infer the number of classes for non one-hot and one-hot labels\n",
    "c = max(dataset.label.max().item() + 1, dataset.label.shape[1])\n",
    "d = dataset.graph['node_feat'].shape[1]\n",
    "\n",
    "# whether or not to symmetrize\n",
    "if not args.directed and args.dataset != 'ogbn-proteins':\n",
    "    dataset.graph['edge_index'] = to_undirected(dataset.graph['edge_index'])\n",
    "\n",
    "edge_index, x = dataset.graph['edge_index'], dataset.graph['node_feat']\n",
    "\n",
    "print(f\"num nodes {n} | num edges {edge_index.size(1)} | num classes {c} | num node feats {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8801a333-8344-40cf-a299-7203c44c74b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: NodeFormer(\n",
      "  (convs): ModuleList(\n",
      "    (0-1): 2 x NodeFormerConv(\n",
      "      (Wk): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (Wq): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (Wv): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (Wo): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fcs): ModuleList(\n",
      "    (0): Linear(in_features=14530, out_features=128, bias=True)\n",
      "    (1): Linear(in_features=128, out_features=7, bias=True)\n",
      "  )\n",
      "  (bns): ModuleList(\n",
      "    (0-2): 3 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (chonggou): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=14530, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### Load method ###\n",
    "model = parse_method(args, dataset, n, c, d, device)\n",
    "\n",
    "### Loss function (Single-class, Multi-class) ###\n",
    "if args.dataset in ('yelp-chi', 'deezer-europe', 'twitch-e', 'fb100', 'ogbn-proteins'):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "else:\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "### Performance metric (Acc, AUC, F1) ###\n",
    "if args.metric == 'rocauc':\n",
    "    eval_func = eval_rocauc\n",
    "elif args.metric == 'f1':\n",
    "    eval_func = eval_f1\n",
    "else:\n",
    "    eval_func = eval_acc\n",
    "\n",
    "logger = Logger(args.runs, args)\n",
    "\n",
    "model.train()\n",
    "print('MODEL:', model)\n",
    "\n",
    "adjs = []\n",
    "adj, _ = remove_self_loops(edge_index)\n",
    "adj, _ = add_self_loops(adj, num_nodes=n)\n",
    "adjs.append(adj)\n",
    "for i in range(args.rb_order - 1): # edge_index of high order adjacency\n",
    "    adj = adj_mul(adj, adj, n)\n",
    "    adjs.append(adj)\n",
    "dataset.graph['adjs'] = [adj.to(torch.int).to(torch.int64) for adj in adjs]\n",
    "\n",
    "adj_loss_inter, _ = remove_self_loops(dataset.edge[:, 0:dataset.n_infer])\n",
    "adj_loss_intra2, _ = remove_self_loops(dataset.edge[:, dataset.n_infer:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff6abe76-bed2-4dc2-bfe0-1f60ddfbfb60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_histogram(data, label, name = \"\", bin_width=0.1):\n",
    "    # 将CUDA张量转移到CPU\n",
    "    values = data[0].cpu()\n",
    "    categories = data[1].cpu()\n",
    "    label = label.squeeze().cpu()\n",
    "    \n",
    "    # early stop\n",
    "    # pro = sum(data[0] > 0.8)/dataset.n_data2\n",
    "\n",
    "    # 初始化计数向量\n",
    "    count_vector_1 = torch.zeros(int(1 / bin_width)) # 计p的数目\n",
    "    count_vector_2 = torch.zeros(int(1 / bin_width)) # 计正确的数目\n",
    "\n",
    "    # 判断索引与对应的类别向量中的值是否相等\n",
    "    for i in range(len(values)):\n",
    "        index = int(values[i] / bin_width)\n",
    "        if index < len(count_vector_1):\n",
    "            count_vector_1[index] += 1\n",
    "            if categories[i] == label[i]:\n",
    "                count_vector_2[index] += 1\n",
    "\n",
    "    # 绘制直方图\n",
    "    bins = torch.linspace(0, 1, int(1 / bin_width))\n",
    "    \n",
    "    # plt.figure(figsize=(3, 2), dpi=200)\n",
    "    plt.bar(bins, count_vector_1, width=bin_width, align='edge', color='blue', label='False')\n",
    "    plt.bar(bins, count_vector_2, width=bin_width, align='edge', color='orange', label='True', alpha=0.7)\n",
    "    plt.xlabel('Probability')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.savefig('savefig_example.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6040a86d-8383-4dda-8eb3-34f7398faa05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Training loop ###\n",
    "l = 100 # 用于早停\n",
    "num = 0\n",
    "\n",
    "for run in range(args.runs):\n",
    "    split_idx = split_idx_lst[run]\n",
    "    rna_idx = split_idx['train']\n",
    "    atac_idx = torch.arange(dataset.n_data1, n)\n",
    "    train_atac_idx = torch.empty(0)\n",
    "    label_train = dataset.label.squeeze(1).clone()\n",
    "    # num_batch1 = rna_idx.size(0) // args.num_batch\n",
    "    # num_batch2 = atac_idx.size(0) // args.num_batch\n",
    "    \n",
    "    model.reset_parameters()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),weight_decay=args.weight_decay, lr=args.lr)\n",
    "    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 200], gamma=0.5)\n",
    "    best_val = float('-inf')\n",
    "    \n",
    "    \n",
    "    for epoch in range(args.epochs):\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "\n",
    "        num_batch1 = rna_idx.size(0) // args.num_batch\n",
    "        num_batch2 = atac_idx.size(0) // args.num_batch\n",
    "        num_batch3 = train_atac_idx.size(0) // args.num_batch\n",
    "        \n",
    "        # dataset.label = dataset.label.to(device)\n",
    "        label_train = label_train.to(device)\n",
    "\n",
    "        idx1 = torch.randperm(rna_idx.size(0)) # 生成随机序列\n",
    "        idx2 = torch.randperm(atac_idx.size(0)) # 生成随机序列\n",
    "        idx3 = torch.randperm(train_atac_idx.size(0)) # 生成随机序列\n",
    "        \n",
    "        L = 0\n",
    "        \n",
    "        for i in range(args.num_batch):\n",
    "            idx_i_rna = rna_idx[idx1[i*num_batch1:(i+1)*num_batch1]]\n",
    "            idx_i_atac = atac_idx[idx2[i*num_batch2:(i+1)*num_batch2]]\n",
    "            idx_i_train_atac = train_atac_idx[idx3[i*num_batch3:(i+1)*num_batch3]]\n",
    "            \n",
    "            idx_i = torch.cat((idx_i_rna, idx_i_train_atac, idx_i_atac), dim=0).long()\n",
    "            x_i = x[idx_i].to(device)\n",
    "            adjs_i = []\n",
    "            sub_edge_inter, _, inter_mask_i = subgraph(idx_i, adj_loss_inter, num_nodes=n, relabel_nodes=True, return_edge_mask=True)\n",
    "            sub_edge_intra2, _, intra2_mask_i = subgraph(idx_i, adj_loss_intra2, num_nodes=n, relabel_nodes=True, return_edge_mask=True)\n",
    "            edge_index_i, _, edge_mask_i = subgraph(idx_i, adjs[0], num_nodes=n, relabel_nodes=True, return_edge_mask=True)\n",
    "            adjs_i.append(edge_index_i.to(device))\n",
    "            for k in range(args.rb_order - 1):\n",
    "                edge_index_i, _ = subgraph(idx_i, adjs[k+1], num_nodes=n, relabel_nodes=True)\n",
    "                adjs_i.append(edge_index_i.to(device))\n",
    "            optimizer.zero_grad()\n",
    "            out_i, link_loss_, z1, z2, chonggou = model(x_i, adjs_i, args.tau)\n",
    "            out_i = F.log_softmax(out_i, dim=1)\n",
    "            p = F.softmax(out_i, dim=1)\n",
    "        \n",
    "            idx_cross_entropy = torch.cat((idx_i_rna, idx_i_train_atac), dim=0).long()\n",
    "        \n",
    "            loss1 = criterion(out_i[0:idx_cross_entropy.shape[0]], label_train[idx_cross_entropy])\n",
    "            # loss -= args.lamda * sum(link_loss_) / len(link_loss_)\n",
    "            \n",
    "            # 计算训练后锚点对间的MSE损失。希望锚点对尽量的接近\n",
    "            node1_inter = sub_edge_inter[0].int()\n",
    "            node2_inter = sub_edge_inter[1].int()\n",
    "            feature1_inter = z2[:,node1_inter, :] ###\n",
    "            feature2_inter = z2[:,node2_inter, :] ###\n",
    "            mse_loss_inter = F.mse_loss(feature1_inter, feature2_inter)\n",
    "            mse_loss_inter = torch.clamp(mse_loss_inter - 0.1, min=0)\n",
    "            loss2 = (mse_loss_inter)# + mse_loss_intra\n",
    "        \n",
    "            # data2图内连接损失\n",
    "            values, indices = torch.max(p, dim=1)\n",
    "            node1_intra = sub_edge_intra2[0]\n",
    "            node2_intra = sub_edge_intra2[1]\n",
    "            node1_values = values[node1_intra.int()]\n",
    "            node2_values = values[node2_intra.int()]\n",
    "            node1_indices = indices[node1_intra.int()]\n",
    "            node2_indices = indices[node2_intra.int()]\n",
    "            a = node1_values * node2_values * (node1_indices == node2_indices).float()\n",
    "            a[a > 0.001] = 1\n",
    "            loss_inter2 = 1 - torch.sum(a) / len(a)\n",
    "            loss3 = torch.clamp(loss_inter2 - 0.1, min=0)\n",
    "        \n",
    "            loss = loss1 + loss2 + loss3\n",
    "            L += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "\n",
    "        if epoch % 5 == 0: #args.eval_step\n",
    "            result = evaluate_cpu(model, dataset, split_idx, eval_func, criterion, args)\n",
    "            logger.add_result(run, result[:-1])\n",
    "\n",
    "            print(f'Epoch: {epoch:02d}, '\n",
    "                  f'Loss: {loss:.4f}, '\n",
    "                  f'交叉熵: {loss1:.4f}, '\n",
    "                  f'Hard正则: {loss2:.4f}, '\n",
    "                  f'graph2正则: {loss3:.4f}, '\n",
    "                  f'Train: {100 * result[0]:.2f}%, ,'\n",
    "                  f'Query Criterion Loss: {result[2]:.4f}, '\n",
    "                  f'Query: {100 * result[1]:.2f}%')\n",
    "            if epoch % 50 == 0:\n",
    "                model.eval() # 评估模式\n",
    "                model.to(torch.device(\"cpu\"))\n",
    "                out, link_loss_, z1, z2, chonggou = model(x, adjs, args.tau)\n",
    "                p = F.softmax(out, dim=1)\n",
    "                \n",
    "                selected_indices = torch.nonzero(torch.max(p, axis = 1)[0][atac_idx] > 0.95).squeeze()\n",
    "                if (selected_indices.numel()):\n",
    "                    if atac_idx[selected_indices].dim() == 0:\n",
    "                        query_selected = atac_idx[selected_indices].unsqueeze(0)\n",
    "                    else:\n",
    "                        query_selected = atac_idx[selected_indices]\n",
    "                    train_atac_idx = torch.cat((train_atac_idx, query_selected), dim=0)\n",
    "                \n",
    "                    label_pre = torch.max(p, axis=1)[1][query_selected].to(device)\n",
    "                    print(\"123: \",sum(label_train[query_selected] == label_pre)/(1+int(label_train[query_selected].size(0))))\n",
    "                    print(\"数目: \", int(label_train[query_selected].size(0)), \". 正确的数目: \", sum(label_train[query_selected] == label_pre))\n",
    "\n",
    "                    label_train[query_selected] = label_pre\n",
    "\n",
    "                    # 从 idx_atac 的索引中去掉已移动的索引\n",
    "                    mask = torch.ones_like(atac_idx, dtype=torch.bool)\n",
    "                    mask[selected_indices] = 0\n",
    "                    # 通过布尔掩码筛选保留的元素\n",
    "                    atac_idx = atac_idx[mask]\n",
    "                  \n",
    "                plt.figure(figsize=(3,2))\n",
    "                plot_histogram(torch.max(p[dataset.n_data1:,:], axis=1), dataset.label[dataset.n_data1:], name = \"query\", bin_width=0.1)\n",
    "                model.to(device)\n",
    "                \n",
    "        if l > L/3:\n",
    "            l = L/3\n",
    "            num = 0\n",
    "            ## 保存最优结果\n",
    "            \n",
    "            resultname = args.data_dir + 'results/'\n",
    "            modelname = args.data_dir + 'model/'\n",
    "            \n",
    "            if not os.path.exists(resultname):\n",
    "                os.makedirs(resultname)\n",
    "            if not os.path.exists(modelname):\n",
    "                os.makedirs(modelname)\n",
    "            \n",
    "            filename = args.data_dir + f'results/{args.dataset}.csv'\n",
    "            \n",
    "            model.eval() # 评估模式\n",
    "            model.to(torch.device(\"cpu\"))\n",
    "            out, link_loss_, z1, z2, chonggou = model(x, adjs, args.tau)\n",
    "            p = F.softmax(out, dim=1)\n",
    "            \n",
    "            query_acc = sum(dataset.label.squeeze().numpy()[dataset.n_data1:] == p.argmax(dim=-1, keepdim=True).detach().squeeze().numpy()[dataset.n_data1:])/len(dataset.label.squeeze().numpy()[dataset.n_data1:]);\n",
    "            \n",
    "            # print(f\"Saving results to {filename}\")\n",
    "            torch.save(z2, resultname + 'embedding.pt')\n",
    "            torch.save(p, resultname + 'out.pt')\n",
    "            torch.save(dataset.label, resultname + 'label.pt')\n",
    "            torch.save(dataset.num_celltype, resultname + 'num_celltype.pt')\n",
    "            torch.save(dataset.metadata, resultname + 'metadata.pt')\n",
    "            torch.save(default_args, resultname + 'args.pt')\n",
    "            torch.save(model.state_dict(), modelname + f'{args.dataset}-{args.method}.pkl')\n",
    "            with open(f\"{filename}\", 'w') as write_obj:\n",
    "                write_obj.write(f\"{args.method},\" +\n",
    "                                f\"epoch: {epoch:02d},\" +\n",
    "                                f\"Query: {100 * result[1]:.2f}%\")\n",
    "            model.to(device)\n",
    "        else:\n",
    "            num = num + 1\n",
    "        \n",
    "        # pro = sum(torch.max(p[dataset.n_data1:,:], axis=1)[0] > 0.8)/dataset.n_data2\n",
    "        # print(pro)\n",
    "        if num >= 30:# or pro > 0.8:\n",
    "            print (f'Answer Query: {100 * query_acc:.2f}%')\n",
    "            break;\n",
    "        \n",
    "        \n",
    "#     logger.print_statistics(run)\n",
    "\n",
    "# results = logger.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc7dfd1-3ea6-4577-bc9d-08c35bde852f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 画图\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# adjs = [adj.to(device) for adj in adjs]\n",
    "# x = x.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(modelname + f'{args.dataset}-{args.method}.pkl'))\n",
    "model.to(torch.device(\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "out_i, link_loss_, z1, z2, chonggou = model(x, adjs, args.tau)\n",
    "out_i = F.log_softmax(out_i, dim=1)\n",
    "p = F.softmax(out_i, dim=1)\n",
    "\n",
    "embedding = z2.cpu()\n",
    "embedding = embedding[0].detach().numpy()#\n",
    "\n",
    "label = dataset.label.cpu() # 原始的类型\n",
    "label = label.squeeze().numpy() \n",
    "\n",
    "metadata = dataset.metadata.cpu()\n",
    "metadata = metadata.squeeze().numpy() \n",
    "\n",
    "out = p.cpu() # 预测的类型\n",
    "pre = out.argmax(dim=-1, keepdim=True).detach().squeeze().numpy()\n",
    "\n",
    "num_celltype = dataset.num_celltype # 数字与类型的对应关系\n",
    "print(num_celltype)\n",
    "\n",
    "import umap\n",
    "umap_model = umap.UMAP(n_neighbors=5, n_components=2, metric='euclidean')\n",
    "embedding_tsne = umap_model.fit_transform(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f93d95a-4523-4107-a51f-319a98eaebc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# celltype着色\n",
    "%matplotlib inline\n",
    "labels = np.unique(label)  # 获取唯一的标签值\n",
    "for i in labels:\n",
    "    plt.scatter(embedding_tsne[label == i, 0], embedding_tsne[label == i, 1], s=0.5, label=str(i))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77382171-b04d-4c71-9226-6cb9b0feee36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pre着色\n",
    "%matplotlib inline\n",
    "pres = np.unique(pre)  # 获取唯一的标签值\n",
    "for i in pres:\n",
    "    plt.scatter(embedding_tsne[pre == i, 0], embedding_tsne[pre == i, 1], s=0.5, label=str(i))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e806fe2-eca3-4db1-9791-023921857bcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tech着色\n",
    "%matplotlib inline\n",
    "metadatas = np.unique(metadata)  # 获取唯一的标签值\n",
    "for i in metadatas:\n",
    "    plt.scatter(embedding_tsne[metadata == i, 0], embedding_tsne[metadata == i, 1], s=0.01, label=str(i))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d020c1ae-6648-4d45-9ddf-5ba8e2471058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score,silhouette_samples, accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "## 轮廓系数\n",
    "sil_type = silhouette_samples(np.array(embedding_tsne), label)\n",
    "sil_omic = silhouette_samples(np.array(embedding_tsne), metadata)\n",
    "sil_f1 = (\n",
    "            2\n",
    "            * (1 - (sil_omic + 1) / 2)\n",
    "            * (sil_type + 1)\n",
    "            / 2\n",
    "            / (1 - (sil_omic + 1) / 2 + (sil_type + 1) / 2)\n",
    "        )\n",
    "sil_type.mean(), sil_omic.mean(), sil_f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8a2ff54-feb0-4957-8a01-ca9e49ad72ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Baso', 1: 'Bcell', 2: 'CD4', 3: 'CD8', 4: 'CLP', 5: 'CMP', 6: 'DC', 7: 'GMP', 8: 'HSC/MPP', 9: 'LMPP', 10: 'MEP', 11: 'Mono', 12: 'NK', 13: 'NaiveT', 14: 'early-Ery', 15: 'late-Ery', 16: 'pDC', 17: 'pro/pre-B'}\n"
     ]
    }
   ],
   "source": [
    "## 保存\n",
    "embedding = torch.load(resultname + 'embedding.pt', map_location=torch.device('cpu'))\n",
    "embedding = embedding[0].detach().numpy()\n",
    "\n",
    "label = torch.load(resultname + 'label.pt', map_location=torch.device('cpu')) # 原始的类型\n",
    "label = label.squeeze().numpy() \n",
    "\n",
    "out = torch.load(resultname + 'out.pt', map_location=torch.device('cpu')) # 预测的类型\n",
    "pre = out.argmax(dim=-1, keepdim=True).detach().squeeze().numpy()\n",
    "pro = out.max(dim=-1, keepdim=True)[0].detach().squeeze().numpy()\n",
    "\n",
    "metadata = torch.load(resultname + 'metadata.pt', map_location=torch.device('cpu')) # tech\n",
    "metadata = metadata.squeeze().numpy() \n",
    "\n",
    "num_celltype = torch.load(resultname + 'num_celltype.pt', map_location=torch.device('cpu')) # 数字与类型的对应关系\n",
    "print(num_celltype)\n",
    "\n",
    "pro = out.max(dim=-1, keepdim=True)[0].detach().squeeze().numpy()\n",
    "celltype = [num_celltype[i] for i in label]\n",
    "pre = [num_celltype[i] for i in pre]\n",
    "umap1 = embedding_tsne[:,0]\n",
    "umap2 = embedding_tsne[:,1]\n",
    "metadata = [\"scATAC-seq\" if i == 2 else \"scRNA-seq\" for i in metadata]\n",
    "import pandas as pd\n",
    "p = {   'UMAP1' : umap1,\n",
    "        'UMAP2' : umap2,\n",
    "        'celltype' : celltype,\n",
    "        'pre' : pre,\n",
    "        'tech' : metadata,\n",
    "        'pro' : pro}\n",
    "p = pd.DataFrame(p)\n",
    "p.to_csv(resultname + 'data_list.csv', index=False)\n",
    "p2 = p[p['tech'] == \"scATAC-seq\"]\n",
    "p2.to_csv(resultname + 'data2_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ba469-b389-435b-a20c-e8cb896e998b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sum(p2['pre'] == p2['celltype'])/len(p2['pre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11ad605-2aeb-4aa2-9c86-94652a961361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sum(dataset.label.squeeze().numpy()[dataset.n_data1:] == out.argmax(dim=-1, keepdim=True).detach().squeeze().numpy()[dataset.n_data1:])/len(dataset.label.squeeze().numpy()[dataset.n_data1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
